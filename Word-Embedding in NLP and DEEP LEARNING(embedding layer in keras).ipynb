{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries USed Tensorflow> 2.0  and keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tensorflow >2.0\n",
    "# first you have tensorflow background greater than 2..\n",
    "from tensorflow.keras.preprocessing.text import one_hot # here we implement one_hot.. what the use of it, for this refer to copy notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we have sentences where we perform some one_hot and embedding\n",
    "sent=[  'the glass of milk',\n",
    "     'the glass of juice',\n",
    "     'the cup of tea',\n",
    "    'I am a good boy',\n",
    "     'I am a good developer',\n",
    "     'understand the meaning of words',\n",
    "     'your videos are good',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the glass of milk',\n",
       " 'the glass of juice',\n",
       " 'the cup of tea',\n",
       " 'I am a good boy',\n",
       " 'I am a good developer',\n",
       " 'understand the meaning of words',\n",
       " 'your videos are good']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent # check it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Vocabulary size\n",
    "voc_size=10000\n",
    "# its first step declare vocabulory, my vocab is 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#One Hot Representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the glass of milk\n",
      "the glass of juice\n",
      "the cup of tea\n",
      "I am a good boy\n",
      "I am a good developer\n",
      "understand the meaning of words\n",
      "your videos are good\n",
      "[[8041, 8904, 8698, 8958], [8041, 8904, 8698, 927], [8041, 7524, 8698, 6981], [5356, 5445, 5653, 3728, 5903], [5356, 5445, 5653, 3728, 9926], [8448, 8041, 5747, 8698, 8944], [5258, 8036, 601, 3728]]\n"
     ]
    }
   ],
   "source": [
    "# so here we convert our sentence into one-hot representations...\n",
    "onehot_repr=[]\n",
    "for words in sent: # \n",
    "    print(words) \n",
    "    \n",
    "    onehot_repr.append(one_hot(words,voc_size)) \n",
    "# its function that convert words into one-hot \n",
    "# here two parameter 1st is words which you want to convert into one-hot and another is voc_size \n",
    "# so after apply this one index no assign to our word.\n",
    "# so its provide index no  to the word from our dictionary..\n",
    "print(onehot_repr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Word Embedding Represntation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding # import embedding layer , its convert one-hot representaation into embedding vectors.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # import pad_sequence \n",
    "# why its very very important ,whenever you pass the sentences to the embedding layer, all the sentences should have the same no.of words,\n",
    "#becoz its help to create very good embedding matrix.so for do same size sentence with use pad sequences\n",
    "from tensorflow.keras.models import Sequential # its for init the sequential model ,here we use sequential model in order to create with repecting the embedding layer to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0 8041 8904 8698 8958]\n",
      " [   0    0    0    0 8041 8904 8698  927]\n",
      " [   0    0    0    0 8041 7524 8698 6981]\n",
      " [   0    0    0 5356 5445 5653 3728 5903]\n",
      " [   0    0    0 5356 5445 5653 3728 9926]\n",
      " [   0    0    0 8448 8041 5747 8698 8944]\n",
      " [   0    0    0    0 5258 8036  601 3728]]\n"
     ]
    }
   ],
   "source": [
    "sent_length=8\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "# in pad_sequence 1st parameter is my one_hot representation sentence ,2nd is padding ='pre ', it mean o put in before the indexs\n",
    "# 3rd is maxlen its mean total size of one-hot representation after performing padding\n",
    "# for more go in my  google colab accound and check NLP python file of tokenizations\n",
    "# if i dont pass 3rd parameter it will take by default size , by defualt size is larger sentece size..\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=10\n",
    "# its dimensional of embedding\n",
    "# its feature , and one words convert into 10 vector size.\n",
    "# to understand this refer to copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential() # here init the deep learning sequential model\n",
    "model.add(Embedding(voc_size,10,input_length=sent_length)) #here i add embedding layer , my first layer of ANN is embedding laayer.\n",
    "# 1st parameter is vocabulory size , 2nd parameter is dimesional , 3rd parameter is sentence length, so what is our each and every sentence lenght is 8\n",
    "model.compile('adam','mse') # its compilee with adam optimizer and performing matrix is mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 8, 10)             100000    \n",
      "=================================================================\n",
      "Total params: 100,000\n",
      "Trainable params: 100,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# its my model.summmery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.02565583  0.00791759 -0.02996886  0.03961018 -0.01456248\n",
      "   -0.04630274 -0.02392167  0.04627511 -0.01218774  0.03797766]\n",
      "  [-0.02791396 -0.01811381  0.04219855 -0.01401313  0.01106535\n",
      "    0.04672335 -0.04907656  0.02396217 -0.01073527 -0.01459684]\n",
      "  [-0.01583047 -0.02237868 -0.00142431  0.04683676 -0.0438031\n",
      "    0.02135683 -0.00830128  0.03181615 -0.00306803 -0.01186572]\n",
      "  [-0.00576218  0.02043792  0.02381966 -0.02797967 -0.03179201\n",
      "   -0.02199775 -0.03596029 -0.0224496  -0.04020458  0.04396253]]\n",
      "\n",
      " [[-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.02565583  0.00791759 -0.02996886  0.03961018 -0.01456248\n",
      "   -0.04630274 -0.02392167  0.04627511 -0.01218774  0.03797766]\n",
      "  [-0.02791396 -0.01811381  0.04219855 -0.01401313  0.01106535\n",
      "    0.04672335 -0.04907656  0.02396217 -0.01073527 -0.01459684]\n",
      "  [-0.01583047 -0.02237868 -0.00142431  0.04683676 -0.0438031\n",
      "    0.02135683 -0.00830128  0.03181615 -0.00306803 -0.01186572]\n",
      "  [-0.04485939  0.00485717  0.03013116 -0.00964522  0.03746754\n",
      "   -0.00218707 -0.03605856  0.04106246  0.0265621  -0.02720501]]\n",
      "\n",
      " [[-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.02565583  0.00791759 -0.02996886  0.03961018 -0.01456248\n",
      "   -0.04630274 -0.02392167  0.04627511 -0.01218774  0.03797766]\n",
      "  [-0.04756069  0.02549106 -0.0050577  -0.03104171  0.01850991\n",
      "   -0.02286582 -0.00617414  0.03798502  0.03396164 -0.02362908]\n",
      "  [-0.01583047 -0.02237868 -0.00142431  0.04683676 -0.0438031\n",
      "    0.02135683 -0.00830128  0.03181615 -0.00306803 -0.01186572]\n",
      "  [-0.02485865  0.00077619 -0.03875712  0.0033416   0.03768618\n",
      "    0.02863835  0.03656435 -0.03964634  0.01973028  0.02069051]]\n",
      "\n",
      " [[-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [ 0.04836119  0.02706516  0.0273099  -0.03787792  0.04005631\n",
      "    0.0058253  -0.02983029 -0.04942919 -0.00352923  0.02421066]\n",
      "  [-0.00162209 -0.01741172 -0.0315545   0.0350776  -0.02570046\n",
      "   -0.02848719  0.04549794  0.03539219 -0.00074162  0.00950887]\n",
      "  [ 0.04887208  0.03400368  0.02637709 -0.02524508  0.02186832\n",
      "   -0.03901341 -0.00242839 -0.02655279  0.03890201  0.02749823]\n",
      "  [-0.03575926  0.02845352  0.00985865  0.02836644  0.03423232\n",
      "   -0.03975769 -0.04513068 -0.04498854 -0.00743205  0.00365131]\n",
      "  [-0.01738031  0.02192633 -0.01119532  0.02370665 -0.02535995\n",
      "    0.00632406 -0.00612179  0.04889451 -0.00211214 -0.04561687]]\n",
      "\n",
      " [[-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [ 0.04836119  0.02706516  0.0273099  -0.03787792  0.04005631\n",
      "    0.0058253  -0.02983029 -0.04942919 -0.00352923  0.02421066]\n",
      "  [-0.00162209 -0.01741172 -0.0315545   0.0350776  -0.02570046\n",
      "   -0.02848719  0.04549794  0.03539219 -0.00074162  0.00950887]\n",
      "  [ 0.04887208  0.03400368  0.02637709 -0.02524508  0.02186832\n",
      "   -0.03901341 -0.00242839 -0.02655279  0.03890201  0.02749823]\n",
      "  [-0.03575926  0.02845352  0.00985865  0.02836644  0.03423232\n",
      "   -0.03975769 -0.04513068 -0.04498854 -0.00743205  0.00365131]\n",
      "  [-0.0272638   0.03731311 -0.0030034   0.0490295  -0.03589183\n",
      "   -0.04577992  0.00514235  0.04295862  0.01284554 -0.03938322]]\n",
      "\n",
      " [[-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03627787 -0.0111284  -0.04075663 -0.01167179 -0.02727057\n",
      "   -0.01823688  0.01333408  0.0345926   0.01464761 -0.032591  ]\n",
      "  [-0.02565583  0.00791759 -0.02996886  0.03961018 -0.01456248\n",
      "   -0.04630274 -0.02392167  0.04627511 -0.01218774  0.03797766]\n",
      "  [ 0.01260663 -0.03230705 -0.02889544  0.03993959  0.00399351\n",
      "    0.03983465 -0.04781871  0.01115191 -0.04174561  0.03401455]\n",
      "  [-0.01583047 -0.02237868 -0.00142431  0.04683676 -0.0438031\n",
      "    0.02135683 -0.00830128  0.03181615 -0.00306803 -0.01186572]\n",
      "  [ 0.03246773 -0.02587708  0.03809026  0.04230012 -0.03685371\n",
      "   -0.04935331  0.01282542 -0.04022153  0.04768885 -0.00334449]]\n",
      "\n",
      " [[-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513\n",
      "   -0.01278513 -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      "  [ 0.03750158  0.04104428  0.0153131   0.03723992 -0.00683845\n",
      "    0.03503482 -0.02401035  0.04359097 -0.02274202 -0.01290787]\n",
      "  [-0.00238705 -0.03923496 -0.03286671  0.03652993 -0.04224744\n",
      "   -0.01713107  0.02708001 -0.03562211  0.00083588  0.00552968]\n",
      "  [-0.00231142 -0.0057703  -0.00038049 -0.03494509  0.02883874\n",
      "    0.00179686  0.03742808 -0.00517557 -0.0184718   0.04886205]\n",
      "  [-0.03575926  0.02845352  0.00985865  0.02836644  0.03423232\n",
      "   -0.03975769 -0.04513068 -0.04498854 -0.00743205  0.00365131]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(embedded_docs))\n",
    "# now here in order to see, how my words actually converted\n",
    "# suppose we want to convert this 0    0    0    0 8041 8904 8698 8958 words convert into feature vector, for this we have to do \n",
    "# model.predict(here we give our index sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0, 8041, 8904, 8698, 8958])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_docs[0] # here to understand clearly , we take first indexes sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513 -0.01278513\n",
      "  -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      " [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513 -0.01278513\n",
      "  -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      " [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513 -0.01278513\n",
      "  -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      " [-0.03791308  0.02003479 -0.03470116 -0.02139171 -0.02203513 -0.01278513\n",
      "  -0.00932705  0.01903306 -0.03308139  0.02439653]\n",
      " [-0.02565583  0.00791759 -0.02996886  0.03961018 -0.01456248 -0.04630274\n",
      "  -0.02392167  0.04627511 -0.01218774  0.03797766]\n",
      " [-0.02791396 -0.01811381  0.04219855 -0.01401313  0.01106535  0.04672335\n",
      "  -0.04907656  0.02396217 -0.01073527 -0.01459684]\n",
      " [-0.01583047 -0.02237868 -0.00142431  0.04683676 -0.0438031   0.02135683\n",
      "  -0.00830128  0.03181615 -0.00306803 -0.01186572]\n",
      " [-0.00576218  0.02043792  0.02381966 -0.02797967 -0.03179201 -0.02199775\n",
      "  -0.03596029 -0.0224496  -0.04020458  0.04396253]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(embedded_docs)[0])\n",
    "#here you see my single words into 10 dimesional  and 10 features\n",
    "# here single word convert into my 10 dimesional vector\n",
    "# for example this 8904 words convert into 10 dimesional is  [-0.02791396 -0.01811381  0.04219855 -0.01401313  0.01106535  0.04672335\n",
    "  # -0.04907656  0.02396217 -0.01073527 -0.01459684] .\n",
    "# to undertand this go to copy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in embedding layer still more things like word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
